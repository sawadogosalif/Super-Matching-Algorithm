{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee1f6d4",
   "metadata": {},
   "source": [
    "# Strategy for text treatment\n",
    "\n",
    "**Language detection** - for now the default language is English and we switcht to French if cld3 detects it. Should we consider other languages too?\n",
    "\n",
    "**Remove punctuation and special characters**\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "**Stop-word removal** - stop-word removal is language-based and is done before the stemming, otherwise they might not be detected\n",
    "\n",
    "**Stemming** - stemming is performed in favor for lemmatization, as we're going to be working mainly with names and not even entire sentences. Since lemmatizing depends on the sentence context, it would not be a good option here.\n",
    "\n",
    "**ASCII folding**\n",
    "As we want to make the text prep functions more flexible and allow the passing of some arguments such as custom blacklist of words or custom regex replaces, the new approach is to curry the function with those parameters and then apply it on a Spark DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6096d0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Salif\n",
      "[nltk_data]     SAWADOGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Salif\n",
      "[nltk_data]     SAWADOGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowball stemmer was chosen in favor of Porter Stemmer which is a bit more aggressive and tends to remove too much from a word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# unidecode is the library needed for ASCII folding\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "# Compact Language Detector v3 is a very fast and performant algorithm by Google for language detection\n",
    "import nltk\n",
    "import re\n",
    "import pyspark.sql.functions as F\n",
    "from typing import List, Dict, Optional, Callable\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f342f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./function_tools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa02623",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf('string')\n",
    "@function_vectorizer\n",
    "def get_arabic(text_input: str) -> Optional[str]:\n",
    "    \"\"\"This function parses a string an returns only the arabic characters based\n",
    "    on their unicode values.\n",
    " \n",
    "    Arguments:\n",
    "        text_input {str} -- The string to be parsed.\n",
    " \n",
    "    Returns:\n",
    "        Optional[str] -- The arabic characters discovered or None if the input was empty.\n",
    "    \"\"\"\n",
    "    k = ''\n",
    "    if text_input is None:\n",
    "      return None\n",
    "    for ch in text_input:\n",
    "      if ('\\u0600' <= ch <= '\\u06FF' or\n",
    "        '\\u0750' <= ch <= '\\u077F' or\n",
    "        '\\u08A0' <= ch <= '\\u08FF' or\n",
    "        '\\uFB50' <= ch <= '\\uFDFF' or\n",
    "        '\\uFE70' <= ch <= '\\uFEFF' or\n",
    "        '\\U00010E60' <= ch <= '\\U00010E7F' or\n",
    "        '\\U0001EE00' <= ch <= '\\U0001EEFF' or\n",
    "         ch == \" \"):\n",
    "        k = k + ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66d63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf('string')\n",
    "@function_vectorizer\n",
    "def get_latin(text_input: str) -> Optional[str] :\n",
    "    \"\"\"This function parses a string an returns only the latin characters based\n",
    "    on their unicode values.\n",
    " \n",
    "    Arguments:\n",
    "        text_input {str} -- The string to be parsed.\n",
    " \n",
    "    Returns:\n",
    "        Optional[str] -- The latin characters discovered or None if the input was empty.\n",
    "    \"\"\"\n",
    "    k = ''\n",
    "    if text_input is None:\n",
    "      return None\n",
    "    for ch in text_input:\n",
    "      if ('\\u0000' <= ch <= '\\u007F' or\n",
    "          '\\u0080' <= ch <= '\\u00FF' or\n",
    "          '\\u0100' <= ch <= '\\u017F' or\n",
    "          '\\u0180' <= ch <= '\\u024F' or\n",
    "          '\\u1E00' <= ch <= '\\u1EFF' or\n",
    "          ch == \" \"):\n",
    "        k = k + ch\n",
    "    return k.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a9d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf('string')\n",
    "@function_vectorizer\n",
    "def translate_text(text_input: str, language: str = \"en\",\n",
    "    subscription_key: str = \"2fe8ca9b5c5b459982d622eb088f831a\") -> str:\n",
    "    \"\"\"Make a call to Microsoft's Text Translation API in order to translate\n",
    "    the text input. \n",
    "    \n",
    "    Arguments:\n",
    "        text_input {str} -- The text to be translated\n",
    "        \n",
    "    Keyword Arguments:\n",
    "        language {str} -- The language code for the translation output.\n",
    "        subscription_key {str} -- The subscription key for the service.\n",
    "    \n",
    "    Returns:\n",
    "        str -- The translation of input_text into the language specified\n",
    "        in 'langugage'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if text_input is None or text_input == \"\":\n",
    "      return text_input\n",
    "    \n",
    "    base_url = 'https://api-nam.cognitive.microsofttranslator.com'\n",
    "    path = '/translate?api-version=3.0'\n",
    "    params = '&to=' + language\n",
    "    constructed_url = base_url + path + params\n",
    " \n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    " \n",
    "    # You can pass more than one object in body.\n",
    "    body = [{\n",
    "        'text' : text_input\n",
    "    }]\n",
    "    response = requests.post(constructed_url, headers=headers, json=body)\n",
    "    return response.json()[0]['translations'][0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca99add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_prep_func(word_blacklist: List[str] = [],\n",
    "                        regex_replace: Dict[str, str] = {}\n",
    "                       ) -> Callable:\n",
    "  @F.pandas_udf(\"string\")\n",
    "  @function_vectorizer\n",
    "  def text_prep_sdf(s: str) -> str:\n",
    "    \"\"\"The Spark DataFrame version of the function does essentially\n",
    "    the same. However, in order for the function to work, all the downloads\n",
    "    need to be done in its body, otherwise the Spark cluster would have\n",
    "    no access to the needed component. The download would be done only once\n",
    "    anyways.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      STOPWORDS_EN = stopwords.words(\"english\")\n",
    "      STOPWORDS_FR = stopwords.words(\"french\")\n",
    "      STEMMER_EN = SnowballStemmer(language='english')\n",
    "      STEMMER_FR = SnowballStemmer(language='french')\n",
    "    except:\n",
    "      nltk.download(\"punkt\")\n",
    "      nltk.download(\"stopwords\")\n",
    "      STOPWORDS_EN = stopwords.words(\"english\")\n",
    "      STOPWORDS_FR = stopwords.words(\"french\")\n",
    "      STEMMER_EN = SnowballStemmer(language='english')\n",
    "      STEMMER_FR = SnowballStemmer(language='french')\n",
    " \n",
    "    if s is None or s==\"\":\n",
    "      return \"\"\n",
    "#     STOPWORDS_EN = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "#     STOPWORDS_FR = ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
    "    stop_words = STOPWORDS_EN + word_blacklist\n",
    "    stemmer = STEMMER_EN\n",
    " \n",
    "    s = s.lower()\n",
    " \n",
    "    # check if the language is French\n",
    "    s_lang = detect(s)\n",
    "    if s_lang==\"fr\":\n",
    "      stop_words = STOPWORDS_FR + word_blacklist\n",
    "      stemmer = STEMMER_FR\n",
    " \n",
    "    s_clean = s.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    s_tokens = word_tokenize(s_clean)\n",
    "    s_tokens_no_stop = [word for word in s_tokens if word not in stop_words]\n",
    "    s_tokens_stemmed = [stemmer.stem(word) for word in s_tokens_no_stop]\n",
    "    s_ascii = unidecode(\" \".join(s_tokens_stemmed))\n",
    "    \n",
    "    for regex, replace in regex_replace.items():\n",
    "      s_ascii = re.sub(regex, replace, s_ascii)\n",
    "    return(s_ascii.strip())\n",
    "  \n",
    "  return text_prep_sdf\n",
    "\n",
    "@F.pandas_udf(\"string\")\n",
    "@function_vectorizer\n",
    "def ta_remove_address_tail(address: str) -> str:\n",
    "  if address is None:\n",
    "    return None\n",
    "  address_split = address.split(\",\")\n",
    "  if len(address_split) <= 1:\n",
    "    return address\n",
    "  return \", \".join(address_split[:-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
