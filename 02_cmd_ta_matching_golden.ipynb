{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c897bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found numpy version \"1.20.2\" installed with pyspark version \"3.0.2\". Some functions will not work well with this combination of numpy version \"1.20.2\" and pyspark version \"3.0.2\". Please try to upgrade pyspark version to 3.1 or above, or downgrade numpy version to below 1.20.\n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import string\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import re\n",
    "from langdetect import detect\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b11dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "confSpark = SparkConf().set(\"spark.driver.bindAddress\", \"localhost\")\n",
    "spark = SparkContext(\"local[*]\", \"appname\", conf = confSpark)\n",
    "#spark = SparkSession.builder.master(\"local[*]\").appName(\"URBANICITY\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa58f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Salif\n",
      "[nltk_data]     SAWADOGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Salif\n",
      "[nltk_data]     SAWADOGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run ./geospatial_functions.ipynb\n",
    "%run ./01_text_similarity_functions.ipynb\n",
    "%run ./01_text_prep_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15df360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_column_regex_replace = {r\"\\'\": \"\", r\"\\d{5}\": \"\", r\"\\s+\": \" \"}\n",
    "address_column_blacklist = []\n",
    "address_column_regex_replace = {r\"\\'\": \"\", r\"\\s+\": \" \", \"avenu \": \"av \", \"boulevard \": \"bd \"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75fa429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = spark.read.option(\"header\", \"True\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "    csv(\"D:/data_quality/data/customer_invoice_tizi_ouzou.csv\") [[\"Client\", \"LONGITUDE\", \"LATITUDE\", \"Nom\", \"Adresse\"]].\\\n",
    "    withColumnRenamed(\"Client\",\"CUSTOMER_COD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d94bae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "|CUSTOMER_COD|       LONGITUDE|        LATITUDE|                 Nom|             Adresse|\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "|       72011|4.10326666666666|36.6073166666666|       MANOUCHE AMAR|    RUE TALA ATHMANE|\n",
      "|       72009|        4.080755|36.6208683333333|       SADAOUI HAMID|       RUE AMIROUCHE|\n",
      "|       72000|         4.08768|       36.617363|     SARL TOUTE ELLA|     RUE IGHIL HAMOU|\n",
      "|       71999|         4.04352|        36.69592|    BOUZERD ABDENOUR|  BENI DOUALA CENTRE|\n",
      "|       71998|        4.178637|       36.768272|           DLIM SAID|RESIDENCE HADJILI...|\n",
      "|       71997|4.05602833333333|36.7153966666666|      ALIOUT  RAZIKA|     RUE ANAR AMELAL|\n",
      "|       71996|      4.03790698|      36.7076863|       MOHDEB CHERIF|      VILLAGE LOUDHA|\n",
      "|       71991|4.07188333333333|        36.64442|          ETS MEDDAD|   LAGAR SIDI MAAMAR|\n",
      "|       71967|        4.077905|          36.716|        LEKHEL FARID|  CITE 104 LOGEMENTS|\n",
      "|       71966|4.07170166666666|36.8012566666666|     BERREFAS CHAFIK|  CITE 600 LOGEMENTS|\n",
      "|       71953|4.07280833333333|36.6958983333333|        LOUNAS NACER|COOP ASSAMEUR 600...|\n",
      "|       71946|4.04055333333333|36.6352333333333|       GHANEM BRAHIM|     CITE 05 JUILLET|\n",
      "|       71945|4.00686833333333|        36.54471|      MESBAHI MOHAND|      SOUK EL THNINE|\n",
      "|       71944|        4.256008|       36.552013|    BENAZOUG IBRAHIM|     CITE MOULDIOUNE|\n",
      "|       71943|      4.02135422|     36.73182946|       HAMOUM BRAHIM|     BOUZGUEN CENTRE|\n",
      "|       71942|         4.01008|36.5927933333333|    IMARAZENE FAREDJ|  VILLAGE IHITOUSSEN|\n",
      "|       71941|4.00483833333333|36.5446833333333|        BESSAHA LYES|    VILLAGE TAKOUCHT|\n",
      "|       71940|4.04400666666666|36.7052266666666|LAHLOUH MOHAND CH...|   VILLAGE AIT ZELAL|\n",
      "|       71936|         4.11735|       36.701945|  BOUBCHIR ABDEREZAK|RUE UNIVERSITÉ TAMDA|\n",
      "|       71935|3.84238833333333|36.5365416666666|BEN AMAR BELKACEM...|       OUACIF CENTRE|\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2722e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|CUSTOMER_COD|\n",
      "+------------+\n",
      "|       56341|\n",
      "|       38014|\n",
      "|       38861|\n",
      "|       54921|\n",
      "|       37832|\n",
      "|       37757|\n",
      "|       41128|\n",
      "|       40777|\n",
      "|       52474|\n",
      "|       37968|\n",
      "|       38055|\n",
      "|       38144|\n",
      "|       64884|\n",
      "|       38015|\n",
      "|       39745|\n",
      "|       45301|\n",
      "|       55586|\n",
      "|       39813|\n",
      "|       62873|\n",
      "|       38867|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd_golden_uri=\"C:/Users/Salif SAWADOGO/OneDrive - EQUATORIAL COCA-COLA BOTTLING COMPANY S.L/dynamic segmentation/matching/output/horeca_tz_customer_subset.csv\"\n",
    "cmd_golden_ids = spark.read.option(\"header\", \"True\").\\\n",
    "               csv(cmd_golden_uri)\\\n",
    "    [[\"CUSTOMER_COD\"]].withColumn(\"CUSTOMER_COD\", F.col(\"CUSTOMER_COD\").cast(\"integer\"))\n",
    "cmd_golden_ids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449646fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_golden_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf374ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_column_blacklist = [\"cafe\", \"cf\", \"restaurant\", \"estaurant\", \"rest\", \"ag\", \"ste\", \"café\", \"snack\", \"hotel\", \"sarl\", \"rotisserie\", \"marrakech\"]\n",
    "name_column_regex_replace = {r\"\\'\": \"\", r\"\\d{5}\": \"\", r\"\\s+\": \" \"}\n",
    "address_column_blacklist = []\n",
    "address_column_regex_replace = {r\"\\'\": \"\", r\"\\s+\": \" \", \"avenu \": \"av \", \"boulevard \": \"bd \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ac047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|CUSTOMER_COD|       LONGITUDE|        LATITUDE|                 Nom|             Adresse|           NOM_CLEAN|       ADRESSE_CLEAN|\n",
      "+------------+----------------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       67250|3.84282333333333|        36.53764|        FOUNES HAKIM|       OUACIF CENTRE|          foun hakim|          ouac centr|\n",
      "|       65126|4.05923833333333|36.7026533333333|         COFFEE TIME|BVD LES BOUZARD K...|          coffe time|bvd les bouzard k...|\n",
      "|       65115|         3.95982|36.7239933333333|         RABHI SAMIR|COMMUNE AIN ZAOUI...|         rabhi samir|commun ain zaoui ...|\n",
      "|       65113|4.36676666666666|36.6663883333333|        SADJI YOUCEF|      MAKOUDA CENTRE|        sadji youcef|       makouda centr|\n",
      "|       65061|         3.95595|36.7342433333333|        AMAR CHERGUI|RUE DES FRERES MA...|        amar chergui|rue des frere mam...|\n",
      "|       64884|4.30798166666666|36.6587733333333|          MALEM FETA|       AZAZGA CENTRE|          malem feta|        azazga centr|\n",
      "|       64458|4.15319166666666|36.7002383333333|       ELBAHRI FARID|      zone sud ouest|       elbahri farid|       zon sud ouest|\n",
      "|       64252|4.48071166666666|36.6106566666666|    OUMATOUS MOULOUD|     RUE YOUCEF OMAR|    oumatous mouloud|     rue youcef omar|\n",
      "|       63796|        4.083242|       36.619366|        HAFED HAKIMA|     RUE LES CHABANE|         hafe hakima|           ru chaban|\n",
      "|       63472|         4.24522|        36.79302|      GAOUAOUI HAKIM|cité des oranges,...|      gaouaoui hakim|cit orang tadm ti...|\n",
      "|       62873|4.36454166666666|36.7462433333333|     AIT ALI HOUCINE|tikobaine tizi ouzou|          ali houcin| tikobain tizi ouzou|\n",
      "|       62693|4.30919833333333|       36.738955|       MOURI RABAH 2|  ENEL COMMUNE FREHA|       mouri rabah 2|    enel commun freh|\n",
      "|       62047|4.31443833333333|        36.78579|          OUHAB AMAR|rue colonel amiro...|          ouhab amar|rue colonel amiro...|\n",
      "|       62029|4.37842833333333|       36.749925|   AIT RAMDANE AMMAR|AGOUNI AUCHARKI C...|        ramdan ammar|agouni aucharki c...|\n",
      "|       61655|4.16647166666666|        36.67668|SELHAOUI SOUAD EP...|couperait  kessra...|selhaou souad ep ...|coup kessraou loc...|\n",
      "|       61321|4.17709333333333|36.6408933333333|     BOUKAROU YACINE|ROUTE AIT AISSA M...|      boukarou yacin|rout ait aissa mi...|\n",
      "|       61219|         4.19592|36.7132083333333|     OUMELLIL TOUFIK|rue principale bo...|     oumellil toufik|rue principal bou...|\n",
      "|       58879|      4.08003931|     36.86121219|       CHABANE NABIL|rue des frères ou...|        chaban nabil|ru frer oumran ti...|\n",
      "|       58872|4.05318666666666|36.7029083333333|     BOUARABE YACINE|   ZONE INDUSTRIELLE|       bouarab yacin|    zone industriell|\n",
      "|       58827|4.07591166666666|        36.64707|      ABKARI  MADJID| CITE BORDJ HASNAOUA|       abkari madjid| cite bordj hasnaoua|\n",
      "+------------+----------------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd_golden = cmd.join(cmd_golden_ids, [\"CUSTOMER_COD\"]) \\\n",
    "                .withColumn(\"NOM_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"Nom\"))) \\\n",
    "                .withColumn(\"ADRESSE_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(F.col(\"Adresse\")))\n",
    "\n",
    "cmd_golden.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "107bfff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------+--------------------+--------------------+\n",
      "|                name|             address|location_lat|location_lon|          name_CLEAN|       address_CLEAN|\n",
      "+--------------------+--------------------+------------+------------+--------------------+--------------------+\n",
      "|         Paris Beach|                null|    35.92549|    0.090527|          pari beach|                    |\n",
      "|    Pecherie Almirez|                null|    35.93473|     0.08014|     pecheri almirez|                    |\n",
      "|Pizza 447 Mostaganem|                null|    35.92259|      0.1093|pizza 447 mostaganem|                    |\n",
      "|             algerie|                null|    35.92785|     0.10009|              algeri|                    |\n",
      "| Bekhchis Mostaganem|                null|    35.91465|    0.056364|  bekhchi mostaganem|                    |\n",
      "|             Happy M|                null|   35.915577|    0.057128|               happi|                    |\n",
      "|Caféteria les jum...|                null|   35.919296|    0.061058|     cafeteri jumeau|                    |\n",
      "|        Pizza Plus +|                null|        null|        null|          pizza plus|                    |\n",
      "|  Piano Sandwicherie|                null|   35.917297|     0.06057|   piano sandwicheri|                    |\n",
      "|           L'Ardoise|                null|    35.93035|     0.07842|              ardois|                    |\n",
      "|           Holy Food|                null|        null|        null|           holi food|                    |\n",
      "| Pecherie El Mansour|                null|    35.93115|     0.08918|   pecher el mansour|                    |\n",
      "|            Bvb Food|                null|        null|        null|            bvb food|                    |\n",
      "|             Sevilla|                null|        null|        null|             sevilla|                    |\n",
      "|          Cool Glace|                null|    35.91474|     0.05654|          cool glace|                    |\n",
      "|            BVB Food|                null|   35.895214|    0.073953|            bvb food|                    |\n",
      "|         Happy diner|                null|        null|        null|         happi diner|                    |\n",
      "|          Restaurant|                null|        null|        null|                    |                    |\n",
      "| Restaurant Equinoxe|Bd Ain Sbaa Ali E...|   34.874912|   -1.324541|             equinox|bd sbaa ali en fa...|\n",
      "|       Quindi Gelato|                null|   35.712074|   -0.570729|       quindi gelato|                    |\n",
      "+--------------------+--------------------+------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tripadvisor_data_uri =\"C:/Users/Salif SAWADOGO/OneDrive - EQUATORIAL COCA-COLA BOTTLING COMPANY S.L/dynamic segmentation/data_acquisition/TripAdvisor/code/output/ta_combined_l3_Algeria_prepped.parquet\"\n",
    "\n",
    "ta = spark.read.option(\"header\", \"True\").option(\"inferSchema\", \"True\").parquet(tripadvisor_data_uri)[[\"id\", \"name\", \"address\", \"location_lat\", \"location_lon\"]] \\\n",
    "          .withColumn(\"name_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"name\"))) \\\n",
    "          .withColumn(\"address_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(ta_remove_address_tail(F.col(\"address\")))) \\\n",
    "          .withColumn(\"location_lat\", F.col(\"location_lat\").cast(\"float\")) \\\n",
    "          .withColumn(\"location_lon\", F.col(\"location_lon\").cast(\"float\"))\n",
    "ta[[\"name\",\"address\",\"location_lat\",\"location_lon\",\"name_CLEAN\", \"address_CLEAN\"]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d37ebc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1237"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e346961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_join(l_sdf,\n",
    "               l_id,\n",
    "               l_lon,\n",
    "               l_lat,\n",
    "               l_name,\n",
    "               l_addr,\n",
    "               r_sdf,\n",
    "               r_id,\n",
    "               r_lon,\n",
    "               r_lat,\n",
    "               r_name,\n",
    "               r_addr,\n",
    "               distance_threshold_m,\n",
    "              ):\n",
    "  l_slice = l_sdf[[l_id, l_lon, l_lat, l_name, l_addr]]\n",
    "  r_slice = r_sdf[[r_id, r_lon, r_lat, r_name, r_addr]]\n",
    "  \n",
    "  l_og_cols = l_slice.columns\n",
    "  r_og_cols = r_slice.columns\n",
    "  for c in l_og_cols:\n",
    "    l_slice = l_slice.withColumnRenamed(c, \"L_\"+c)\n",
    "  for c in r_og_cols:\n",
    "    r_slice = r_slice.withColumnRenamed(c, \"R_\"+c)\n",
    "  inner_joined = l_slice.join(r_slice, haversine_distance_sdf(F.col(\"L_\"+l_lon), F.col(\"L_\"+l_lat), F.col(\"R_\"+r_lon), F.col(\"R_\"+r_lat)) <= distance_threshold_m, how=\"inner\")\n",
    "       \n",
    " \n",
    "# withColumn(\"rank\", F.dense_rank().over(Window.partitionBy('L_' + l_id).orderBy(F.col(\"similarity\").desc()))).filter(\"rank <= 15\")\n",
    "#   l_joined = l_slice.join(inner_joined, l_slice.columns)\n",
    "  return(inner_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0eae287",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = match_join(cmd_golden, \"CUSTOMER_COD\", \"LONGITUDE\", \"LATITUDE\", \"NOM_CLEAN\", \"ADRESSE_CLEAN\", ta, \"id\", \"location_lon\", \"location_lat\", \"name_CLEAN\", \"address_CLEAN\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7403cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_id=\"CUSTOMER_COD\"\n",
    "l_lon=\"LONGITUDE\"\n",
    "l_lat=\"LATITUDE\"\n",
    "l_name=\"NOM_CLEAN\" \n",
    "l_addr=\"ADRESSE_CLEAN\"\n",
    "r_id=\"id\"\n",
    "r_lon=\"location_lon\"\n",
    "r_lat=\"location_lat\"\n",
    "r_name=\"name_CLEAN\"\n",
    "r_addr=\"address_CLEAN\"\n",
    "distance_threshold_m=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fff9aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched=matched.withColumn(\"name_similarity\", compound_similarity_sdf(F.col(\"L_\"+l_name),F.col(\"R_\"+r_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58163bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched=matched.withColumn(\"dist_m\", haversine_distance_sdf(F.col(\"L_\"+l_lon), F.col(\"L_\"+l_lat), F.col(\"R_\"+r_lon), F.col(\"R_\"+r_lat))).\\\n",
    "          withColumn(\"name_similarity\", compound_similarity_sdf(F.col(\"L_\"+l_name),F.col(\"R_\"+r_name))).\\\n",
    "          withColumn(\"address_similarity\", compound_similarity_sdf(F.col(\"L_\"+l_addr),F.col(\"R_\"+r_addr))).\\\n",
    "          withColumn(\"dist_similarity\", (distance_threshold_m - F.col(\"dist_m\"))/distance_threshold_m).\\\n",
    "          withColumn(\"similarity\", F.col(\"name_similarity\")*0.75 + F.col(\"address_similarity\")*0.15 + F.col(\"dist_similarity\")*0.15).withColumn(\"rank\", F.dense_rank().over(Window.partitionBy('L_' + l_id).orderBy(F.col(\"similarity\").desc()))).filter(\"rank <= 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56181015",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1192.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 93, ECCBWS307.eccbc.local, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 258, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 252, in init_stream_yield_batches\n    batch = self._create_batch(series)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 194, in _create_batch\n    arrs.append(create_array(s, t))\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 168, in create_array\n    raise e\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 158, in create_array\n    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)\n  File \"pyarrow\\array.pxi\", line 902, in pyarrow.lib.Array.from_pandas\n  File \"pyarrow\\array.pxi\", line 302, in pyarrow.lib.array\n  File \"pyarrow\\array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow\\error.pxi\", line 97, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Could not convert Column<b'vectorized_function(coffe time, le bagdad)'> with type Column: tried to convert to double\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 258, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 252, in init_stream_yield_batches\n    batch = self._create_batch(series)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 194, in _create_batch\n    arrs.append(create_array(s, t))\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 168, in create_array\n    raise e\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 158, in create_array\n    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)\n  File \"pyarrow\\array.pxi\", line 902, in pyarrow.lib.Array.from_pandas\n  File \"pyarrow\\array.pxi\", line 302, in pyarrow.lib.array\n  File \"pyarrow\\array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow\\error.pxi\", line 97, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Could not convert Column<b'vectorized_function(coffe time, le bagdad)'> with type Column: tried to convert to double\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-a9e74b4d4b07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgolden_standart_uri\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C:/Users/Salif SAWADOGO/OneDrive - EQUATORIAL COCA-COLA BOTTLING COMPANY S.L/dynamic segmentation/matching/output/golden_standardd.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmatched\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"True\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgolden_standart_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\bigdata_Local\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PREMIER\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\bigdata_Local\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PREMIER\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1192.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 93, ECCBWS307.eccbc.local, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 258, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 252, in init_stream_yield_batches\n    batch = self._create_batch(series)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 194, in _create_batch\n    arrs.append(create_array(s, t))\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 168, in create_array\n    raise e\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 158, in create_array\n    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)\n  File \"pyarrow\\array.pxi\", line 902, in pyarrow.lib.Array.from_pandas\n  File \"pyarrow\\array.pxi\", line 302, in pyarrow.lib.array\n  File \"pyarrow\\array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow\\error.pxi\", line 97, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Could not convert Column<b'vectorized_function(coffe time, le bagdad)'> with type Column: tried to convert to double\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 258, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 252, in init_stream_yield_batches\n    batch = self._create_batch(series)\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 194, in _create_batch\n    arrs.append(create_array(s, t))\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 168, in create_array\n    raise e\n  File \"C:\\bigdata_Local\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\serializers.py\", line 158, in create_array\n    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)\n  File \"pyarrow\\array.pxi\", line 902, in pyarrow.lib.Array.from_pandas\n  File \"pyarrow\\array.pxi\", line 302, in pyarrow.lib.array\n  File \"pyarrow\\array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow\\error.pxi\", line 97, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Could not convert Column<b'vectorized_function(coffe time, le bagdad)'> with type Column: tried to convert to double\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "golden_standart_uri=\"C:/Users/Salif SAWADOGO/OneDrive - EQUATORIAL COCA-COLA BOTTLING COMPANY S.L/dynamic segmentation/matching/output/golden_standardd.csv\"\n",
    "matched.repartition(1).write.mode(\"overwrite\").option(\"header\", \"True\").csv(golden_standart_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
