{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport databricks.koalas as ks\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import string\n",
    "import re\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"URBANICITY\").getOrCreate()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0822d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_parquet_uri = \"/mnt/AA/ba008/cmd/transformed/marrakech_city_200706_100000_cmd.parquet\"\n",
    "cmd_channels = [\"HOTEL\", \"RECA\", \"SNACK\"]\n",
    "cmd_columns = [\"CUSTOMER_ID\", \"LONGITUDE\", \"LATITUDE\", \"CUSTOMER\", \"STREET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta_parquet_uri = \"/mnt/AA/ba008/tripadvisor/transformed/marrakech_city_restaurants_and_hotels_to_match.parquet\"\n",
    "ta_parquet_uri =\"/mnt/AA/ba008/tripadvisor/outputs/ta_combined_l3_morocco_prepped_20201015.parquet\"\n",
    "ta_columns = [\"id\", \"name\", \"address\", \"location_lat\", \"location_lon\"]\n",
    " \n",
    "# ta_no_geo_parquet_uri = \"/mnt/AA/ba008/tripadvisor/transformed/marrakech_no_geolocation_restaurants_and_hotels_to_match.parquet\"\n",
    "ta_no_geo_parquet_uri = ta_parquet_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7696be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the string cleaning functions \n",
    "name_column_blacklist = [\"cafe\", \"cf\", \"restaurant\", \"estaurant\", \"rest\", \"ag\", \"ste\", \"cafÃ©\", \"snack\", \"sn\", \"hotel\", \"sarl\", \"rotisserie\", \"marrakech\", \"management\"]\n",
    "name_column_regex_replace = {r\"\\'\": \"\", r\"\\d{5}\": \"\", r\"\\s+\": \" \"}\n",
    "address_column_blacklist = []\n",
    "address_column_regex_replace = {r\"\\'\": \"\", r\"\\s+\": \" \", \"avenu \": \"av \", \"boulevard \": \"bd \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24ea1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Salif\n",
      "[nltk_data]     SAWADOGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Salif\n",
      "[nltk_data]     SAWADOGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run ./01_text_prep_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed95432",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./geospatial_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_join(l_sdf: pyspark.sql.DataFrame,\n",
    "               l_id: str,\n",
    "               l_lon: str,\n",
    "               l_lat: str,\n",
    "               l_name: str,\n",
    "               l_addr: str,\n",
    "               r_sdf: pyspark.sql.DataFrame,\n",
    "               r_id: str,\n",
    "               r_lon: str,\n",
    "               r_lat: str,\n",
    "               r_name: str,\n",
    "               r_addr: str,\n",
    "               distance_threshold_m: float = 0,\n",
    "               similarity_threshold: float = 0.59,\n",
    "               name_similarity_weight: float = 0.75,\n",
    "               address_similarity_weight: float = 0.15,\n",
    "               dist_similarity_weight: float = 0.1,\n",
    "               no_geolocation: bool = False\n",
    "              ) -> pyspark.sql.DataFrame:\n",
    "  \"\"\"This function performs a name and address similarity based matching with an\n",
    "  optional geolocation filtering (if coordinates are available) between two sources.\n",
    "  The left (first) source is treated as the primary one even though the results produced\n",
    "  are 1-to-1 in the sense that each item from the right (second) source is matched to just\n",
    "  one item from the left source. In case an item from the right source is the best match\n",
    "  for multiple items from the left source, the tie is broken by choosing the left item\n",
    "  which has the highest similarity score. The matching loop continues until no further\n",
    "  matches like that could be done.\n",
    " \n",
    "  Args:\n",
    "      l_sdf (pyspark.sql.DataFrame): The first source to be matched.\n",
    "      l_id (str): The name of the primary key column of the first source.\n",
    "      l_lon (str): The name of the longitude column of the first source.\n",
    "      l_lat (str): The name of the latitude column of the first source.\n",
    "      l_name (str): The name of the item name column of the first source.\n",
    "      l_addr (str): The name of the item address column of the first source.\n",
    "      r_sdf (pyspark.sql.DataFrame): The second source to be matched.\n",
    "      r_id (str): The name of the primary key column of the second source.\n",
    "      r_lon (str): The name of the longitude column of the second source.\n",
    "      r_lat (str): The name of the latitude column of the second source.\n",
    "      r_name (str): The name of the item name column of the second source.\n",
    "      r_addr (str): The name of the item address column of the second source.\n",
    "      distance_threshold_m (float, optional): When geolocation is available, the radius in meters within which items\n",
    "        from the second source will be matched to items from the first source. Defaults to 0.\n",
    "      similarity_threshold (float, optional): The similarity threshold above which a matched pair is accepted and\n",
    "        below which the match is rejected. Defaults to 0.59.\n",
    "      name_similarity_weight (float, optional): The weight given to the name similarity. Defaults to 0.75.\n",
    "      address_similarity_weight (float, optional): The weight given to the address similarity. Defaults to 0.15.\n",
    "      dist_similarity_weight (float, optional): The weight given to the distance similarity (geographical closeness). Defaults to 0.1.\n",
    "      no_geolocation (bool, optional): Flags whether the geolocation filtering should be performed. Defaults to False.\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame: A Spark DataFrame with the 1-1 matches including the input columns and some of the numerical\n",
    "        results such as the different similarity scores.\n",
    "  \"\"\"\n",
    "  l_slice = l_sdf[[l_id, l_lon, l_lat, l_name, l_addr]]\n",
    "  r_slice = r_sdf[[r_id, r_lon, r_lat, r_name, r_addr]]\n",
    "  \n",
    "  l_og_cols = l_slice.columns\n",
    "  r_og_cols = r_slice.columns\n",
    "  for c in l_og_cols:\n",
    "    l_slice = l_slice.withColumnRenamed(c, \"L_\"+c)\n",
    "  for c in r_og_cols:\n",
    "    r_slice = r_slice.withColumnRenamed(c, \"R_\"+c)\n",
    "  \n",
    "  if not no_geolocation:\n",
    "    fully_joined = l_slice.join(r_slice, haversine_distance_sdf(F.col(\"L_\"+l_lon), F.col(\"L_\"+l_lat), F.col(\"R_\"+r_lon), F.col(\"R_\"+r_lat)) <= distance_threshold_m, how=\"inner\")\n",
    "    similarity_expr = F.col(\"name_similarity\")*name_similarity_weight + F.col(\"address_similarity\")*address_similarity_weight + F.col(\"dist_similarity\")*dist_similarity_weight\n",
    "  else:\n",
    "    fully_joined = l_slice.crossJoin(r_slice)\n",
    "    similarity_expr = F.col(\"name_similarity\")*name_similarity_weight + F.col(\"address_similarity\")*address_similarity_weight\n",
    "  fully_joined = fully_joined \\\n",
    "                          .withColumn(\"dist_m\", haversine_distance_sdf(F.col(\"L_\"+l_lon), F.col(\"L_\"+l_lat), F.col(\"R_\"+r_lon), F.col(\"R_\"+r_lat))) \\\n",
    "                          .withColumn(\"name_similarity\", compound_similarity_sdf(F.col(\"L_\"+l_name),F.col(\"R_\"+r_name))) \\\n",
    "                          .withColumn(\"address_similarity\", compound_similarity_sdf(F.col(\"L_\"+l_addr),F.col(\"R_\"+r_addr))) \\\n",
    "                          .withColumn(\"dist_similarity\", (distance_threshold_m - F.col(\"dist_m\"))/distance_threshold_m) \\\n",
    "                          .withColumn(\"similarity\", similarity_expr) \\\n",
    "                          .withColumn(\"rank\", F.dense_rank().over(Window.partitionBy('L_' + l_id).orderBy(F.col(\"similarity\").desc()))) #.persist()\n",
    "  \n",
    "  first_run = True\n",
    "  while True:\n",
    "    top_ranks = fully_joined.filter(\"rank == 1\").filter(F.col(\"similarity\") >= similarity_threshold)\n",
    "    r_max_similarity = top_ranks[\"R_\"+r_id, \"similarity\"].groupby(\"R_\"+r_id).max().withColumnRenamed(\"max(similarity)\", \"similarity\")\n",
    "    if first_run:\n",
    "      result = top_ranks.join(r_max_similarity, on=[\"R_\"+r_id, \"similarity\"])\n",
    "      if result.rdd.isEmpty():\n",
    "        return result\n",
    "      first_run = False\n",
    "    else:\n",
    "      running_result = top_ranks.join(r_max_similarity, on=[\"R_\"+r_id, \"similarity\"])\n",
    "      if running_result.rdd.isEmpty():\n",
    "        return result\n",
    "      result = result.union(running_result)\n",
    "    exclusion_ids = result[[\"L_\"+l_id, \"R_\"+r_id]].toPandas()\n",
    "    l_exclude = list(exclusion_ids[\"L_\"+l_id])\n",
    "    r_exclude = list(exclusion_ids[\"R_\"+r_id])\n",
    "    fully_joined = fully_joined.filter(~F.col(\"L_\"+l_id).isin(l_exclude)) \\\n",
    "                               .filter(~F.col(\"R_\"+r_id).isin(r_exclude)) \\\n",
    "                               .withColumn(\"rank\", F.dense_rank().over(Window.partitionBy('L_' + l_id).orderBy(F.col(\"similarity\").desc()))) #.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = spark.read.parquet(cmd_parquet_uri).where(F.col(\"CHANNEL_CUSTOM\").isin(cmd_channels))[cmd_columns]\n",
    "cmd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bdf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ry:\n",
    "  cmd_clean = cmd.withColumn(\"CUSTOMER_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"CUSTOMER\"))) \\\n",
    "                 .withColumn(\"STREET_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(F.col(\"STREET\")))\n",
    "except:\n",
    "  cmd_clean = cmd.withColumn(\"CUSTOMER_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"CUSTOMER\"))) \\\n",
    "                 .withColumn(\"STREET_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(F.col(\"STREET\")))\n",
    "cmd_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a023b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = spark.read.parquet(ta_parquet_uri).filter(F.col(\"location_lat\").isNotNull())[ta_columns]\n",
    "ta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_clean = ta.withColumn(\"name_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"name\"))) \\\n",
    "             .withColumn(\"address_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(ta_remove_address_tail(F.col(\"address\"))))\n",
    "ta_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = match_join(cmd_clean, \"CUSTOMER_ID\", \"LONGITUDE\", \"LATITUDE\", \"CUSTOMER_CLEAN\", \"STREET_CLEAN\", ta_clean, \"id\", \"location_lon\", \"location_lat\", \"name_CLEAN\", \"address_CLEAN\", 2000, 0.59).persist()\n",
    "matched.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched.write.mode(\"overwrite\").parquet(\"/mnt/AA/ba008/data_samples/matching/marrakech_city_matching_with_geoloc_20201015.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1168bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_no_geo = spark.read.parquet(ta_no_geo_parquet_uri).filter(F.col(\"location_lat\").isNull())[ta_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  ta_no_geo_clean = ta_no_geo.withColumn(\"name_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"name\"))) \\\n",
    "                             .withColumn(\"address_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(ta_remove_address_tail(F.col(\"address\"))))\n",
    "except:\n",
    "  ta_no_geo_clean = ta_no_geo.withColumn(\"name_CLEAN\", make_text_prep_func(word_blacklist=name_column_blacklist, regex_replace=name_column_regex_replace)(F.col(\"name\"))) \\\n",
    "                           .withColumn(\"address_CLEAN\", make_text_prep_func(word_blacklist=address_column_blacklist, regex_replace=address_column_regex_replace)(ta_remove_address_tail(F.col(\"address\"))))\n",
    "ta_no_geo_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_no_geo_not_matched = ta_no_geo_clean.filter(~F.col(\"id\").isin(list(matched[[\"R_id\"]].toPandas())))\n",
    "ta_no_geo_not_matched.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021c835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_not_matched = cmd_clean.filter(~F.col(\"CUSTOMER_ID\").isin(list(matched[[\"L_CUSTOMER_ID\"]].toPandas())))\n",
    "cmd_not_matched.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf85dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_no_geo = match_join(cmd_not_matched, \"CUSTOMER_ID\", \"LONGITUDE\", \"LATITUDE\", \"CUSTOMER_CLEAN\", \"STREET_CLEAN\", ta_no_geo_not_matched, \"id\", \"location_lon\", \"location_lat\", \"name_CLEAN\", \"address_CLEAN\", 0, 0.65, 0.8, 0.2, 0, no_geolocation=True).persist()\n",
    "matched_no_geo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a20bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_no_geo.write.mode(\"overwrite\").parquet(\"/mnt/AA/ba008/data_samples/matching/marrakech_city_matching_no_geoloc_20201015.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
