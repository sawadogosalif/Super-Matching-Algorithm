{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6bd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Callable, List\n",
    " \n",
    "import numpy as np\n",
    "import pyspark\n",
    "import shapely\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyspark.sql.types import BooleanType\n",
    "from shapely.geometry import Polygon, Point\n",
    "from geopandas import GeoDataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"URBANICITY\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649f28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./function_tools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b9c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(coord1: float, coord2: float) -> float:\n",
    "    if coord1 is None or coord2 is None:\n",
    "        return None\n",
    "    # Coordinates in decimal degrees (e.g. 43.60, -79.49)\n",
    "    lon1, lat1 = coord1\n",
    "    lon2, lat2 = coord2\n",
    "    R = 6371000  # radius of Earth in meters\n",
    "    phi_1 = np.radians(lat1)\n",
    "    phi_2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a),np.sqrt(1 - a))\n",
    "    meters = R * c  # output distance in meters\n",
    "#     km = meters / 1000.0  # output distance in kilometers\n",
    "    meters = round(meters)\n",
    "#     km = round(km, 3)\n",
    "    #print(f\"Distance: {meters} m\")\n",
    "    #print(f\"Distance: {km} km\")\n",
    "    return meters\n",
    "haversine_sdf = F.pandas_udf(function_vectorizer(haversine), \"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b28e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(longit_a: float, latit_a: float,\n",
    "        longit_b: float, latit_b: float) -> float:\n",
    "    \"\"\"This function takes the geocoordinates of 2 points and returns the \n",
    "    haversine distance between them, measured in meters.\n",
    "    \n",
    "    Arguments:\n",
    "        longit_a {float} -- The longitude of the first point in decimal\n",
    "            coordinates.\n",
    "        latit_a {float} -- The latitude of the first point in decimal\n",
    "            coordinates.\n",
    "        longit_b {float} -- The longitude of the second point in decimal\n",
    "            coordinates.\n",
    "        latit_b {float} -- The latitude of the second point in decimal\n",
    "            coordinates.\n",
    "    \n",
    "    Returns:\n",
    "        float -- The Haversine (large circle) distance between the two points.\n",
    "    \"\"\"\n",
    " \n",
    "    # Transform to radians\n",
    "    longit_a, latit_a, longit_b, latit_b = map(np.radians, [longit_a,  latit_a, longit_b, latit_b])\n",
    "    dist_longit = longit_b - longit_a\n",
    "    dist_latit = latit_b - latit_a\n",
    "    # Calculate area\n",
    "    area = np.sin(dist_latit/2)**2 + np.cos(latit_a) * np.cos(latit_b) * np.sin(dist_longit/2)**2\n",
    "    # Calculate the central angle\n",
    "    central_angle = 2 * np.arcsin(np.sqrt(area))\n",
    "    #   central_angle = 2 * np.arctan2(np.sqrt(area), np.sqrt(1-area))\n",
    "    radius = 6371000\n",
    "    # Calculate Distance\n",
    "    distance = central_angle * radius\n",
    "    return abs(round(distance, 2))\n",
    "  \n",
    "haversine_distance_sdf = F.pandas_udf(function_vectorizer(haversine_distance), \"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7553d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_osm(sdf: pyspark.sql.dataframe.DataFrame,\n",
    "               contains_any: Optional[Union[str, List[str]]] = None,\n",
    "               contains_all: Optional[Union[str, List[str]]] = None,\n",
    "               contains_neither: Optional[Union[str, List[str]]] = None,\n",
    "               preserve: List[str] = [\"osm_id\", \"latitude\", \"longitude\"],\n",
    "               minimal: bool = True) -> pyspark.sql.dataframe.DataFrame:\n",
    "  \"\"\"This function takes a Spark Data Frame with a transformed OSM data set\n",
    "  (meaning in long tabular format, with the columns 'osm_id', 'latitude' and\n",
    "  'longitude' present), applies some basic filtering, and returns the filtered\n",
    "  set. For simplicity, the only filtering options are: (contains_any OR \n",
    "  contains_all) AND contains_neither.\n",
    "  \n",
    "  Args:\n",
    "      sdf (pyspark.sql.dataframe.DataFrame): The starting transformed OSM data frame.\n",
    "      contains_any (Optional[Union[str, List[str]]], optional): String or list of strings, specifying\n",
    "        records with which OSM tag/values should be left in the filtered set. They need to contain ANY\n",
    "        of the string values to satisfy the contidion. Can't be used together with 'contains_all'.\n",
    "        Defaults to None.\n",
    "      contains_all (Optional[Union[str, List[str]]], optional): String or list of strings, specifying\n",
    "        records with which OSM tag/values should be left in the filtered set. They need to contain ALL\n",
    "        of the string values to satisfy the contidion. Can't be used together with 'contains_any'.\n",
    "        Defaults to None.\n",
    "      contains_neither (Optional[Union[str, List[str]]], optional): String or list of strings, specifying\n",
    "        records with which OSM tag/values should NOT be left in the filtered set. They MUST NOT contain\n",
    "        ANY of the string values to satisfy the condition. Defaults to None.\n",
    "      preserve (List[str], optional): Specify which columns should not be used for the term search for\n",
    "        inclusion/exclusion. Defaults to [\"osm_id\", \"latitude\", \"longitude\"].\n",
    "      minimal (bool, optional): If set to True, only the columns mentioned in 'preserve' will be included\n",
    "        in the returned Data Frame. Defaults to True.\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.dataframe.DataFrame: Contains only the records which satisfy the conditions of\n",
    "        containing the string in 'contains_any' or 'contains_all' and, optionally, NOT containsing the\n",
    "        strings in 'contains_neither'.\n",
    "        \n",
    "  TODO: Raise appropriate errors in place of 'print(\"Error\")'.\n",
    "  \"\"\"\n",
    "  \n",
    "  if contains_any is not None and contains_all is not None:\n",
    "    print(\"Only one of these must be set\")\n",
    "  \n",
    "  # concat everything except for the columns to be preserved\n",
    "  sdf_concat = sdf.withColumn('__concatenated', F.concat_ws(' ',*set(sdf.columns).difference(set(preserve))))\n",
    "  \n",
    "  # build the filter on contains 'either' or 'all'\n",
    "  contains_filter = None\n",
    "  if contains_any is not None:\n",
    "    if isinstance(contains_any, str):\n",
    "      contains_filter = F.col(\"__concatenated\").contains(contains_any)\n",
    "    elif isinstance(contains_any, list):\n",
    "      contains_filter = F.col(\"__concatenated\").contains(contains_any[0])\n",
    "      for i in range(1, len(contains_any)):\n",
    "        contains_filter = contains_filter | F.col(\"__concatenated\").contains(contains_any[i])\n",
    "    else:\n",
    "      print(\"Error\")\n",
    "  elif contains_all is not None:\n",
    "    if isinstance(contains_all, str):\n",
    "      contains_filter = F.col(\"__concatenated\").contains(contains_all)\n",
    "    elif isinstance(contains_all, list):\n",
    "      contains_filter = F.col(\"__concatenated\").contains(contains_all[0])\n",
    "      for i in range(1, len(contains_all)):\n",
    "        contains_filter = contains_filter & F.col(\"__concatenated\").contains(contains_all[i])\n",
    "    else:\n",
    "      print(\"Error\")\n",
    "  else:\n",
    "    print(\"Error\")\n",
    "  \n",
    "  # build the filter on not containing\n",
    "  contains_not_filter = None\n",
    "  if contains_neither is not None:\n",
    "    if isinstance(contains_neither, str):\n",
    "      contains_not_filter = ~F.col(\"__concatenated\").contains(contains_neither)\n",
    "    elif isinstance(contains_neither, list):\n",
    "      contains_not_filter = F.col(\"__concatenated\").contains(contains_neither[0])\n",
    "      for i in range(1, len(contains_neither)):\n",
    "        contains_not_filter = contains_not_filter | F.col(\"__concatenated\").contains(contains_neither[i])\n",
    "      contains_not_filter = ~contains_not_filter\n",
    "    else:\n",
    "      print(\"Error\")\n",
    "  \n",
    "  sdf_filtered = sdf_concat.filter(contains_filter)\n",
    "  if contains_not_filter is not None:\n",
    "    sdf_filtered = sdf_filtered.filter(contains_not_filter)\n",
    "    \n",
    "  if minimal:\n",
    "    return sdf_filtered.select(preserve)\n",
    "  \n",
    "  return sdf_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3043d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_to_gdf(sdf: pyspark.sql.dataframe.DataFrame,\n",
    "               longitude: str = \"longitude\",\n",
    "               latitude: str = \"latitude\",\n",
    "               crs: str = \"epsg:4326\"\n",
    "              ) -> GeoDataFrame:\n",
    "  \n",
    "  pdf = sdf.toPandas()\n",
    "  gdf = gpd.GeoDataFrame(pdf, geometry=gpd.points_from_xy(pdf[longitude], pdf[latitude]), crs=crs)\n",
    "  return(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107efc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_aggregate(base_gdf: GeoDataFrame,\n",
    "                  add_gdf: GeoDataFrame,\n",
    "                  base_id: str,\n",
    "                  add_id: str,\n",
    "                  aggfunc: Callable,\n",
    "                  new_col_name: str\n",
    "                 ) -> Optional[GeoDataFrame]:\n",
    "  gjoin = gpd.sjoin(base_gdf[[base_id, \"geometry\"]], add_gdf[[add_id, \"geometry\"]], how=\"inner\")\n",
    "  if gjoin.shape[0]==0:\n",
    "    return None\n",
    "  agg = pd.pivot_table(gjoin, index=base_id, aggfunc={add_id: aggfunc}).reset_index()[[base_id, add_id]].rename(columns={add_id: new_col_name})\n",
    "  res = base_gdf.merge(agg, how=\"left\", on=\"CUSTOMER_ID\")\n",
    "  res[new_col_name] = res[new_col_name].fillna(0)\n",
    "  return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d880e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_buffer_meters(gdf: GeoDataFrame, geometry_col: str, buffer_col: str, output_geometry_col: str, buffer_factor: float = 1., crs_calc: str = \"epsg:3857\", crs_out: str = \"epsg:4326\") -> GeoDataFrame:\n",
    "  gdf[output_geometry_col] = gdf[geometry_col].to_crs(crs_calc).buffer(gdf[buffer_col] * buffer_factor).to_crs(crs_out)\n",
    "  return(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5d0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_raster_stats(gdf: GeoDataFrame, raster_file: str, col_prefix: str, stats: List[str], crs_out: str = \"epsg:4326\") -> GeoDataFrame:\n",
    "  return(GeoDataFrame.from_features(rs.zonal_stats(gdf, raster_file, prefix=col_prefix, stats=stats, geojson_out=True), crs=crs_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9eae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_polygon_filter(polygon: shapely.geometry.Polygon) -> Callable:\n",
    "  @F.pandas_udf(\"boolean\")\n",
    "  @function_vectorizer\n",
    "  def point_in_polygon(longitude: float,\n",
    "                       latitude: float\n",
    "                       ) -> bool:\n",
    "    return polygon.contains(Point(longitude, latitude))\n",
    "  return point_in_polygon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PREMIER",
   "language": "python",
   "name": "premier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
